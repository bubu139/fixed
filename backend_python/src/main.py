# src/main.py (OPTIMIZED VERSION)
import re
import uvicorn
import json
import os
from pathlib import Path
from fastapi import FastAPI, HTTPException
from src.routes.node_progress import router as node_progress_router
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Literal, Optional
import PyPDF2
from docx import Document
from src.models import NodeProgress
from src.supabase_client import supabase

# Import config
from src.ai_config import genai
from src.ai_schemas.chat_schema import ChatInputSchema
from src.services import rag_service

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(node_progress_router)

# ===== OPTIMIZATION 1: CACHED SYSTEM INSTRUCTION =====
# Thay v√¨ g·ª≠i instruction d√†i trong m·ªói request, ta d√πng system_instruction 1 l·∫ßn
CHAT_SYSTEM_INSTRUCTION = """B·∫°n l√† m·ªôt AI gia s∆∞ to√°n h·ªçc THPT l·ªõp 12 Vi·ªát Nam, chuy√™n h∆∞·ªõng d·∫´n h·ªçc sinh T·ª∞ H·ªåC v√† PH√ÅT TRI·ªÇN T∆∞ DUY.

# NGUY√äN T·∫ÆC C·ªêT L√ïI
üéØ **M·ª§C TI√äU**: Gi√∫p h·ªçc sinh t·ª± kh√°m ph√° ki·∫øn th·ª©c, KH√îNG l√†m b√†i gi√∫p h·ªçc sinh
üìö **PH∆Ø∆†NG PH√ÅP**: S·ª≠ d·ª•ng c√¢u h·ªèi g·ª£i m·ªü (Socratic Method) ƒë·ªÉ d·∫´n d·∫Øt t∆∞ duy
üí° **TRI·∫æT L√ù**: "D·∫°y h·ªçc sinh c√°ch c√¢u c√°, kh√¥ng ph·∫£i cho c√°"

---

## KHI H·ªåC SINH G·ª¨I B√ÄI T·∫¨P

### B∆Ø·ªöC 1: PH√ÇN T√çCH C√ÇU TR·∫¢ L·ªúI C·ª¶A H·ªåC SINH (N·∫æU C√ì)
N·∫øu h·ªçc sinh ƒë√£ l√†m b√†i:

‚úÖ **Ghi nh·∫≠n ƒëi·ªÉm t·ªët:**
- "Em l√†m ƒë√∫ng b∆∞·ªõc [X], c√°ch ti·∫øp c·∫≠n n√†y r·∫•t h·ª£p l√Ω!"
- "√ù t∆∞·ªüng s·ª≠ d·ª•ng [c√¥ng th·ª©c/ph∆∞∆°ng ph√°p] l√† ch√≠nh x√°c!"

‚ö†Ô∏è **Ch·ªâ ra ch·ªó c·∫ßn c·∫£i thi·ªán (KH√îNG N√äU TR·ª∞C TI·∫æP SAI ·ªû ƒê√ÇU):**
- "Em xem l·∫°i b∆∞·ªõc [Y], c√≥ ƒëi·ªÅu g√¨ ƒë√≥ ch∆∞a ch√≠nh x√°c nh√©"
- "K·∫øt qu·∫£ n√†y c√≥ v·∫ª ch∆∞a h·ª£p l√Ω. Em th·ª≠ ki·ªÉm tra l·∫°i b∆∞·ªõc t√≠nh [Z]?"

### B∆Ø·ªöC 2: G·ª¢I M·ªû T∆Ø DUY B·∫∞NG C√ÇU H·ªéI D·∫™N D·∫ÆT
üîç **V·ªÅ ph√¢n t√≠ch ƒë·ªÅ:**
- "ƒê·ªÅ b√†i y√™u c·∫ßu em t√¨m g√¨? Cho em bi·∫øt nh·ªØng g√¨?"
- "Em th·ª≠ vi·∫øt l·∫°i ƒë·ªÅ b√†i theo c√°ch hi·ªÉu c·ªßa m√¨nh xem?"

üß© **V·ªÅ l√Ω thuy·∫øt:**
- "D·∫°ng b√†i n√†y thu·ªôc ch·ªß ƒë·ªÅ n√†o em ƒë√£ h·ªçc?"
- "Em c√≤n nh·ªõ c√¥ng th·ª©c/ƒë·ªãnh l√Ω n√†o li√™n quan kh√¥ng?"

### B∆Ø·ªöC 3: CH·ªà G·ª¢I √ù H∆Ø·ªöNG GI·∫¢I (KH√îNG GI·∫¢I CHI TI·∫æT)
üí° **G·ª£i √Ω nh·∫π:**
- "G·ª£i √Ω: Em th·ª≠ [ph√©p bi·∫øn ƒë·ªïi/c√¥ng th·ª©c] xem sao"
- "B√†i n√†y c√≥ th·ªÉ gi·∫£i b·∫±ng 2 c√°ch: [C√°ch 1] ho·∫∑c [C√°ch 2]"

### B∆Ø·ªöC 4: CH·ªà GI·∫¢I CHI TI·∫æT KHI:
‚úîÔ∏è H·ªçc sinh ƒë√£ c·ªë g·∫Øng nh∆∞ng v·∫´n kh√¥ng hi·ªÉu sau 2-3 l·∫ßn g·ª£i √Ω
‚úîÔ∏è H·ªçc sinh Y√äU C·∫¶U T∆Ø·ªúNG MINH: "Th·∫ßy/c√¥ gi·∫£i m·∫´u gi√∫p em"

---

## ƒê·ªäNH D·∫†NG ƒê·∫¶U RA (JSON B·∫ÆT BU·ªòC)

B·∫°n PH·∫¢I tr·∫£ v·ªÅ JSON v·ªõi c·∫•u tr√∫c:
{
  "reply": "Tin nh·∫Øn v·ªõi h·ªçc sinh (Markdown, LaTeX cho c√¥ng th·ª©c)",
  "mindmap_insights": [
    {
      "node_id": "slug-ten-kien-thuc",
      "parent_node_id": "ung-dung-dao-ham",
      "label": "T√™n ki·∫øn th·ª©c",
      "type": "concept",
      "weakness_summary": "M√¥ t·∫£ l·ªó h·ªïng",
      "action_steps": ["B∆∞·ªõc 1", "B∆∞·ªõc 2"]
    }
  ],
  "geogebra": {
    "should_draw": true/false,
    "reason": "L√Ω do c·∫ßn v·∫Ω",
    "prompt": "M√¥ t·∫£ v·∫Ω",
    "commands": ["f(x)=x^2", "A=(1,2)"]
  }
}

**QUY T·∫ÆC:**
1. "reply" ph·∫£i tham chi·∫øu l·ªãch s·ª≠, b·ªï sung l√Ω thuy·∫øt
2. "mindmap_insights": Ch·ªâ t·∫°o khi ph√°t hi·ªán ƒëi·ªÉm y·∫øu m·ªõi
3. "geogebra": Ch·ªâ khi c·∫ßn h√¨nh minh h·ªça (h√†m s·ªë, h√¨nh h·ªçc)
4. LaTeX: $x^2$, $$\\int_0^1 x dx$$
5. Lu√¥n tr·∫£ v·ªÅ JSON thu·∫ßn, KH√îNG markdown
"""

# ===== OPTIMIZATION 2: REUSE MODEL INSTANCE =====
# Kh·ªüi t·∫°o model 1 l·∫ßn duy nh·∫•t v·ªõi system instruction
_chat_model_cache = None

def get_chat_model():
    """Singleton pattern for chat model"""
    global _chat_model_cache
    if _chat_model_cache is None:
        generation_config = {
            "temperature": 0.7,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 8192,
            "response_mime_type": "application/json",
        }
        
        _chat_model_cache = genai.GenerativeModel(
            "gemini-2.5-flash",
            generation_config=generation_config,
            system_instruction=CHAT_SYSTEM_INSTRUCTION,  # Cached instruction
        )
    return _chat_model_cache

# ===== HELPER FUNCTIONS =====

def extract_reply_only(raw_text: str) -> str:
    """Extract reply from JSON-like text"""
    if not raw_text:
        return ""
    text = raw_text.strip()
    match = re.search(r'"reply"\s*:\s*"([^"]*)"', text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text

def clean_json_response(raw_text: str) -> str:
    """Clean and extract JSON from AI response (Robust Version)"""
    if not raw_text:
        return ""

    # 1. Remove Markdown code blocks first
    text = re.sub(r"```json\s*", "", raw_text)
    text = re.sub(r"```\s*", "", text)
    
    # 2. Find the outer-most JSON object
    # T√¨m c·∫∑p ngo·∫∑c nh·ªçn ngo√†i c√πng
    json_match = re.search(r"\{[\s\S]*\}", text)
    if not json_match:
        # Fallback: th·ª≠ t√¨m m·∫£ng [] n·∫øu kh√¥ng th·∫•y object {}
        json_match = re.search(r"\[[\s\S]*\]", text)
    
    if not json_match:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y c·∫•u tr√∫c JSON. Raw: {raw_text[:100]}...")
        return ""

    json_text = json_match.group(0).strip()
    
    # 3. Clean control characters & Smart Quotes
    # X√≥a c√°c k√Ω t·ª± ƒëi·ªÅu khi·ªÉn l·∫°, gi·ªØ l·∫°i newline c∆° b·∫£n n·∫øu c·∫ßn thi·∫øt
    json_text = re.sub(r"[\x00-\x1F\x7F]", " ", json_text) 
    
    # S·ª¨A L·ªñI: Thay th·∫ø smart quotes (d·∫•u nh√°y cong) b·∫±ng d·∫•u nh√°y th·∫≥ng
    json_text = json_text.replace('‚Äú', '"').replace('‚Äù', '"')
    
    # 4. Handle LaTeX Backslash Issues (Quan tr·ªçng cho To√°n)
    # AI th∆∞·ªùng tr·∫£ v·ªÅ "\frac" (l·ªói JSON) thay v√¨ "\\frac" (ƒë√∫ng JSON).
    # Regex n√†y t√¨m c√°c d·∫•u \ KH√îNG ƒëi k√®m v·ªõi c√°c k√Ω t·ª± tho√°t h·ª£p l·ªá c·ªßa JSON.
    # C√°c k√Ω t·ª± tho√°t JSON h·ª£p l·ªá: " \ / b f n r t u
    # Ch√∫ng ta lo·∫°i 'f' ra kh·ªèi danh s√°ch h·ª£p l·ªá ƒë·ªÉ b·∫Øt ƒë∆∞·ª£c \frac -> bi·∫øn th√†nh \\frac
    # Pattern: T√¨m \ m√† ph√≠a sau KH√îNG L√Ä [" \ / b n r t u]
    try:
        # Th·ª≠ parse tr∆∞·ªõc, n·∫øu ƒë∆∞·ª£c th√¨ tr·∫£ v·ªÅ lu√¥n
        json.loads(json_text)
        return json_text
    except json.JSONDecodeError:
        # N·∫øu l·ªói, th·ª≠ s·ª≠a c√°c d·∫•u backslash thi·∫øu escape
        # V√≠ d·ª•: \lim -> \\lim, \frac -> \\frac, \infty -> \\infty
        # Nh∆∞ng gi·ªØ nguy√™n \n, \t, \", \\
        pattern = r'\\(?![\\"/bnrtu])'
        fixed_text = re.sub(pattern, r'\\\\', json_text)
        return fixed_text
# ===== SCHEMAS =====

class MediaPart(BaseModel):
    url: str

class ConversationTurn(BaseModel):
    role: Literal["user", "assistant"]
    content: str

class ChatInputSchema(BaseModel):
    userId: Optional[str] = None
    message: str
    history: List[ConversationTurn] = Field(default_factory=list)
    media: Optional[List[MediaPart]] = None

# ===== OPTIMIZED CHAT ENDPOINT =====

@app.post("/api/chat")
async def handle_chat(request: ChatInputSchema):
    """
    OPTIMIZED: S·ª≠ d·ª•ng cached model v·ªõi system instruction
    """
    try:
        # 1. Get cached model (ch·ªâ kh·ªüi t·∫°o 1 l·∫ßn)
        model = get_chat_model()
        
        # 2. Build history cho ChatSession
        gemini_history = []
        for turn in request.history:
            if not turn.content:
                continue
            mapped_role = "user" if turn.role == "user" else "model"
            gemini_history.append({
                "role": mapped_role,
                "parts": [{"text": turn.content}],
            })

        # 3. Start chat v·ªõi history (model ƒë√£ c√≥ system instruction)
        chat = model.start_chat(history=gemini_history)

        # 4. Prepare user message v·ªõi RAG context
        context_text = ""
        if request.userId:
            print(f"üîç Searching RAG for user {request.userId}...")
            docs = await rag_service.search_similar_documents(
                request.message, request.userId, purpose="chat"
            )
            if docs:
                context_text = "\n\n=== T√ÄI LI·ªÜU THAM KH·∫¢O ===\n"
                for d in docs:
                    context_text += f"- [{d['file_name']}]: {d['content'][:200]}...\n"

        # 5. OPTIMIZATION 3: COMPACT PROMPT (kh√¥ng g·ª≠i l·∫°i instruction)
        user_prompt = f"{context_text}\n\nC√¢u h·ªèi: {request.message}"
        user_parts = [{"text": user_prompt}]

        if request.media:
            for media in request.media:
                user_parts.append({"text": f"[Media: {media.url}]"})

        # 6. Send message (async)
        response = await chat.send_message_async(user_parts)

        raw_text = response.text if hasattr(response, "text") else None
        if not raw_text:
            raise ValueError("Model kh√¥ng tr·∫£ v·ªÅ ph·∫£n h·ªìi")

        # Default response
        mindmap_data = []
        normalized_geogebra = {
            "should_draw": False,
            "reason": "",
            "prompt": request.message,
            "commands": [],
        }

        # Try parse JSON
        try:
            json_candidate = clean_json_response(raw_text)
            if not json_candidate:
                raise ValueError("Kh√¥ng t√¨m th·∫•y JSON h·ª£p l·ªá")

            payload = json.loads(json_candidate)
            reply_text = (
                payload.get("reply")
                or payload.get("message")
                or extract_reply_only(raw_text)
            )

            md = payload.get("mindmap_insights")
            if isinstance(md, list):
                mindmap_data = md

            geogebra_block = payload.get("geogebra") or {}
            normalized_geogebra = {
                "should_draw": bool(geogebra_block.get("should_draw")),
                "reason": geogebra_block.get("reason") or "",
                "prompt": geogebra_block.get("prompt") or request.message,
                "commands": geogebra_block.get("commands")
                if isinstance(geogebra_block.get("commands"), list)
                else [],
            }

        except Exception as e:
            print(f"JSON parse failed: {e}")
            reply_text = extract_reply_only(raw_text)

        return {
            "reply": reply_text,
            "mindmap_insights": mindmap_data,
            "geogebra": normalized_geogebra,
        }

    except Exception as e:
        print(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== ALL SCHEMAS =====

class ProcessDocumentInput(BaseModel):
    userId: str
    documentId: str
    purpose: str = "chat"

class GenerateExercisesInput(BaseModel):
    userId: Optional[str] = None
    topic: str
    difficulty: str = "medium"
    count: int = 3

class SummarizeTopicInput(BaseModel):
    topic: str
    detail_level: str = "medium"

class GeogebraInputSchema(BaseModel):
    request: str
    graph_type: str = "function"

class AnalyzeTestResultInput(BaseModel):
    userId: str
    testAttempt: dict
    weakTopics: List[dict]

class AnalyzeTestResultOutput(BaseModel):
    analysis: str
    strengths: List[str]
    weaknesses: List[str]
    recommendations: List[str]
    suggestedTopics: List[str]

class GenerateAdaptiveTestInput(BaseModel):
    userId: str
    weakTopics: List[str]
    difficulty: str = "medium"
    
class GenerateTestInput(BaseModel):
    userId: Optional[str] = None
    topic: str
    difficulty: str = "medium"
    testType: str = "standard"
    numQuestions: int = 5

class NodeTestRequest(BaseModel):
    userId: Optional[str] = None
    topic: str

# ===== DOCUMENT PROCESSING =====

def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        return text
    except Exception as e:
        print(f"Error reading PDF {pdf_path}: {e}")
        return ""

def extract_text_from_word(docx_path: str) -> str:
    try:
        doc = Document(docx_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    except Exception as e:
        print(f"Error reading Word file {docx_path}: {e}")
        return ""

def extract_text_from_file(file_path: str) -> str:
    file_path_obj = Path(file_path)
    extension = file_path_obj.suffix.lower()
    
    if extension == '.pdf':
        return extract_text_from_pdf(file_path)
    elif extension in ['.docx', '.doc']:
        return extract_text_from_word(file_path)
    else:
        print(f"Unsupported file format: {extension}")
        return ""

def load_reference_materials(folder_path: str, max_files: int = 5) -> str:
    folder = Path(folder_path)
    if not folder.exists():
        print(f"Warning: Folder {folder_path} does not exist")
        return ""
    
    pdf_files = list(folder.glob("*.pdf"))
    docx_files = list(folder.glob("*.docx"))
    doc_files = list(folder.glob("*.doc"))
    
    all_files = (pdf_files + docx_files + doc_files)[:max_files]
    
    if not all_files:
        print(f"Warning: No PDF or Word files found in {folder_path}")
        return ""
    
    combined_text = ""
    for file in all_files:
        print(f"üìÑ Loading: {file.name}")
        text = extract_text_from_file(str(file))
        if text:
            combined_text += f"\n\n=== T√ÄI LI·ªÜU: {file.name} ===\n{text}\n"
    
    return combined_text

# ===== PATHS =====

BASE_DIR = Path(__file__).parent.parent
EXERCISES_FOLDER = BASE_DIR / "reference_materials" / "exercises"
TESTS_FOLDER = BASE_DIR / "reference_materials" / "tests"

EXERCISES_FOLDER.mkdir(parents=True, exist_ok=True)
TESTS_FOLDER.mkdir(parents=True, exist_ok=True)

# ===== CACHED MODELS FOR OTHER ENDPOINTS =====

_exercise_model_cache = None
_test_model_cache = None
_geogebra_model_cache = None
_summarize_model_cache = None

def get_exercise_model():
    global _exercise_model_cache
    if _exercise_model_cache is None:
        _exercise_model_cache = genai.GenerativeModel(
            'gemini-2.5-flash',
            generation_config={"temperature": 0.7},
            system_instruction="B·∫°n l√† m·ªôt AI t·∫°o b√†i t·∫≠p to√°n h·ªçc chuy√™n nghi·ªáp cho h·ªçc sinh THPT l·ªõp 12 Vi·ªát Nam."
        )
    return _exercise_model_cache

def get_test_model():
    global _test_model_cache
    if _test_model_cache is None:
        _test_model_cache = genai.GenerativeModel(
            'gemini-2.5-flash',
            generation_config={"temperature": 0.6, "response_mime_type": "application/json"},
            system_instruction="""B·∫°n l√† chuy√™n gia bi√™n so·∫°n ƒë·ªÅ thi THPT Qu·ªëc gia m√¥n To√°n.
QUY T·∫ÆC:
1. M·ªói c√¢u PH·∫¢I c√≥ ƒë·∫ßy ƒë·ªß d·ªØ li·ªáu
2. S·ª≠ d·ª•ng LaTeX: $x^2$
3. T·∫•t c·∫£ \\ trong LaTeX ph·∫£i escape: \\\\frac, \\\\lim
4. Tr·∫£ v·ªÅ JSON thu·∫ßn, KH√îNG markdown"""
        )
    return _test_model_cache

def get_geogebra_model():
    global _geogebra_model_cache
    if _geogebra_model_cache is None:
        _geogebra_model_cache = genai.GenerativeModel(
            'gemini-2.5-flash',
            generation_config={"temperature": 0.3, "response_mime_type": "application/json"},
            system_instruction="B·∫°n l√† chuy√™n gia GeoGebra. Chuy·ªÉn ƒë·ªïi y√™u c·∫ßu th√†nh l·ªánh GeoGebra h·ª£p l·ªá."
        )
    return _geogebra_model_cache

def get_summarize_model():
    global _summarize_model_cache
    if _summarize_model_cache is None:
        _summarize_model_cache = genai.GenerativeModel(
            'gemini-2.5-flash',
            generation_config={"temperature": 0.5},
            system_instruction="B·∫°n l√† gi·∫£ng vi√™n to√°n h·ªçc chuy√™n t√≥m t·∫Øt ki·∫øn th·ª©c s√∫c t√≠ch."
        )
    return _summarize_model_cache

# ===== ROOT ENDPOINT =====

@app.get("/")
async def root():
    return {
        "status": "ok", 
        "message": "Math Tutor API - OPTIMIZED",
        "model": "gemini-2.5-flash",
        "optimizations": [
            "Cached system instruction",
            "Reused model instances",
            "Compact prompts",
            "Session-based chat"
        ],
        "endpoints": [
            "/api/chat",
            "/api/generate-exercises",
            "/api/generate-test",
            "/api/generate-node-test",
            "/api/process-document",
            "/api/summarize-topic",
            "/api/geogebra",
            "/api/analyze-test-result",
            "/api/generate-adaptive-test"
        ]
    }

# ===== GENERATE EXERCISES ENDPOINT =====

@app.post("/api/generate-exercises")
async def handle_generate_exercises(request: GenerateExercisesInput):
    try:
        print(f"üìö Generating exercises for topic: {request.topic}")
        
        # RAG Integration
        context_text = ""
        if request.userId:
            docs = await rag_service.search_similar_documents(request.topic, request.userId, purpose="test")
            if docs:
                context_text = "\n\n=== T√ÄI LI·ªÜU THAM KH·∫¢O ===\n"
                for d in docs:
                    context_text += f"- {d['content']}\n"
        
        reference_text = load_reference_materials(str(EXERCISES_FOLDER), max_files=3)
        
        model = get_exercise_model()
        
        prompt = f"""T·∫°o {request.count} b√†i t·∫≠p to√°n h·ªçc v·ªÅ ch·ªß ƒë·ªÅ: "{request.topic}"
ƒê·ªô kh√≥: {request.difficulty}

T√†i li·ªáu tham kh·∫£o:
{context_text}
{reference_text}

Y√äU C·∫¶U:
- Ph√π h·ª£p To√°n 12 Vi·ªát Nam
- L·ªùi gi·∫£i chi ti·∫øt t·ª´ng b∆∞·ªõc
- S·ª≠ d·ª•ng LaTeX khi c·∫ßn
- Format Markdown

ƒê·ªãnh d·∫°ng:
## B√†i 1
**ƒê·ªÅ b√†i:** [N·ªôi dung]

**L·ªùi gi·∫£i:**
[Gi·∫£i th√≠ch]

**ƒê√°p √°n:** [K·∫øt qu·∫£]"""
        
        response = model.generate_content(prompt)
        
        if not response or not hasattr(response, 'text'):
            raise ValueError("Model kh√¥ng tr·∫£ v·ªÅ ph·∫£n h·ªìi")
        
        exercises_text = response.text.strip()
        
        return {"exercises": exercises_text}
        
    except Exception as e:
        print(f"‚ùå Generate exercises error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== PROCESS DOCUMENT ENDPOINT =====

@app.post("/api/process-document")
async def process_document(request: ProcessDocumentInput):
    try:
        success = await rag_service.process_document(
            user_id=request.userId,
            document_id=request.documentId,
            purpose=request.purpose
        )
        if not success:
            raise HTTPException(status_code=500, detail="Processing failed")
        return {"status": "ok", "message": "Document processed successfully"}
    except Exception as e:
        print(f"Process document error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== GENERATE NODE TEST ENDPOINT =====

@app.post("/api/generate-node-test")
async def generate_node_test(req: NodeTestRequest):
    try:
        topic = req.topic
        model = get_test_model()

        # RAG Integration
        context_text = ""
        if req.userId:
            docs = await rag_service.search_similar_documents(topic, req.userId, purpose="test")
            if docs:
                context_text = "\n\n=== T√ÄI LI·ªÜU THAM KH·∫¢O ===\n"
                for d in docs:
                    context_text += f"- {d['content']}\n"

        prompt = f"""T·∫°o ƒë·ªÅ ki·ªÉm tra to√°n l·ªõp 12 cho ch·ªß ƒë·ªÅ: "{topic}"

T√ÄI LI·ªÜU:
{context_text}

Y√äU C·∫¶U:
- 15 c√¢u multiple-choice
- 4 c√¢u true-false (m·ªói c√¢u 4 √Ω)
- 2 c√¢u short-answer
- S·ª≠ d·ª•ng LaTeX: $x^2$
- T·∫§T C·∫¢ \\ PH·∫¢I ESCAPE: \\\\frac, \\\\lim, \\\\infty

JSON B·∫ÆT BU·ªòC:
{{
  "title": "KI·ªÇM TRA {topic.upper()}",
  "parts": {{
    "multipleChoice": {{"title": "...", "questions": [...]}},
    "trueFalse": {{"title": "...", "questions": [...]}},
    "shortAnswer": {{"title": "...", "questions": [...]}}
  }}
}}"""

        response = model.generate_content(prompt)
        raw_text = response.text

        try:
            json_text = clean_json_response(raw_text)
            if not json_text:
                raise ValueError("Kh√¥ng t√¨m th·∫•y JSON h·ª£p l·ªá")
            data = json.loads(json_text)
        except Exception as e:
            print(f"‚ùå NODE TEST JSON ERROR: {e}")
            print(f"‚ùå Raw text: {raw_text}")
            raise HTTPException(status_code=500, detail=f"AI tr·∫£ v·ªÅ JSON kh√¥ng h·ª£p l·ªá: {str(e)}")
        
        if "parts" not in data:
            raise HTTPException(status_code=500, detail="Thi·∫øu 'parts' trong JSON")

        return {"topic": topic, "test": data}

    except Exception as e:
        print(f"‚ùå NODE TEST ERROR: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== GENERATE TEST ENDPOINT =====

@app.post("/api/generate-test")
async def handle_generate_test(request: GenerateTestInput):
    try:
        print(f"üìù Generating test for topic: {request.topic}")
        reference_text = load_reference_materials(str(TESTS_FOLDER), max_files=3)
        
        model = get_test_model()
        
        # RAG Integration
        context_text = ""
        if request.userId:
            docs = await rag_service.search_similar_documents(request.topic, request.userId, purpose="test")
            if docs:
                context_text = "\n\n=== T√ÄI LI·ªÜU RAG ===\n"
                for d in docs:
                    context_text += f"- {d['content']}\n"

        # Prompt ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u
        prompt = f"""T·∫°o ƒë·ªÅ ki·ªÉm tra TO√ÅN 12 v·ªÅ: "{request.topic}"
ƒê·ªô kh√≥: {request.difficulty}

T√ÄI LI·ªÜU:
{context_text}
{reference_text if reference_text else ""}

QUY T·∫ÆC:
- M·ªói c√¢u c√≥ ƒë·∫ßy ƒë·ªß d·ªØ li·ªáu
- LaTeX: $x^2$
- T·∫§T C·∫¢ \\ PH·∫¢I ESCAPE: \\\\frac, \\\\lim

C·∫§U TR√öC:
- Ph·∫ßn 1: Tr·∫Øc nghi·ªám 4 l·ª±a ch·ªçn
- Ph·∫ßn 2: ƒê√∫ng/Sai (4 √Ω)
- Ph·∫ßn 3: Tr·∫£ l·ªùi ng·∫Øn

JSON (KH√îNG markdown):
{{
  "title": "...",
  "parts": {{
    "multipleChoice": {{...}},
    "trueFalse": {{...}},
    "shortAnswer": {{...}}
  }}
}}"""
        
        response = model.generate_content(prompt)
        raw_text = response.text
        
        try:
            json_text = clean_json_response(raw_text)
            if not json_text:
                raise ValueError("Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c JSON t·ª´ ph·∫£n h·ªìi c·ªßa AI")
            
            result = json.loads(json_text)
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON parse error: {e}")
            print(f"‚ùå Raw text causing error: {raw_text}") # In ra ƒë·ªÉ debug
            # Fallback: Tr·∫£ v·ªÅ l·ªói 500 nh∆∞ng c√≥ th√¥ng tin
            raise HTTPException(status_code=500, detail=f"L·ªói ƒë·ªçc d·ªØ li·ªáu t·ª´ AI: {str(e)}")
        except ValueError as e:
            print(f"‚ùå Value error: {e}")
            raise HTTPException(status_code=500, detail=str(e))
            
        if "parts" not in result:
             # ƒê√¥i khi AI tr·∫£ v·ªÅ c·∫•u tr√∫c kh√°c, th·ª≠ mapping l·∫°i n·∫øu c√≥ th·ªÉ ho·∫∑c b√°o l·ªói
            raise HTTPException(status_code=500, detail="AI tr·∫£ v·ªÅ thi·∫øu tr∆∞·ªùng 'parts'")
        
        return {
            "topic": request.topic,
            "difficulty": request.difficulty,
            "test": result
        }
        
    except Exception as e:
        print(f"‚ùå Generate test error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== SUMMARIZE TOPIC ENDPOINT =====

@app.post("/api/summarize-topic")
async def handle_summarize_topic(request: SummarizeTopicInput):
    try:
        print(f"üìñ Summarizing topic: {request.topic}")
        
        model = get_summarize_model()
        
        prompt = f"""T√≥m t·∫Øt ch·ªß ƒë·ªÅ sau ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu:
- G·∫°ch ƒë·∫ßu d√≤ng
- LaTeX khi c·∫ßn
- Ti√™u ƒë·ªÅ ph·ª•

Ch·ªß ƒë·ªÅ: {request.topic}
ƒê·ªô chi ti·∫øt: {request.detail_level}"""
        
        response = model.generate_content(prompt)
        summary_text = response.text.strip()
        
        return {"topic": request.topic, "summary": summary_text}
        
    except Exception as e:
        print(f"‚ùå Summarize error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== GEOGEBRA ENDPOINT =====

@app.post("/api/geogebra")
async def handle_geogebra(request: GeogebraInputSchema):
    try:
        model = get_geogebra_model()
        
        prompt = f"""T·∫°o l·ªánh GeoGebra cho: {request.request}

Tr·∫£ v·ªÅ JSON:
{{
  "commands": ["command1", "command2"]
}}"""
        
        response = model.generate_content(prompt)
        json_text = clean_json_response(response.text)
        if not json_text:
            raise ValueError("Kh√¥ng t√¨m th·∫•y JSON h·ª£p l·ªá")
            
        result = json.loads(json_text)
        
        if "commands" not in result:
            raise ValueError("Invalid response format")
        
        return result
        
    except Exception as e:
        print(f"Geogebra error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== ANALYZE TEST RESULT ENDPOINT =====

@app.post("/api/analyze-test-result")
async def handle_analyze_test_result(request: AnalyzeTestResultInput):
    try:
        model = genai.GenerativeModel(
            'gemini-2.5-flash',
            generation_config={"temperature": 0.6},
        )
        
        attempt = request.testAttempt
        weak_topics = request.weakTopics
        
        incorrect_answers_str = ""
        try:
            incorrect_answers = [a for a in attempt['answers'] if not a['isCorrect']]
            
            if not incorrect_answers:
                incorrect_answers_str = "**H·ªçc sinh ƒë√£ tr·∫£ l·ªùi ƒë√∫ng t·∫•t c·∫£!**\n"
            else:
                incorrect_answers_str = "**C√ÅC C√ÇU SAI:**\n"
                for i, ans in enumerate(incorrect_answers[:5]):
                    incorrect_answers_str += f"{i+1}. {ans.get('topic', 'N/A')}\n"
        except Exception as e:
            incorrect_answers_str = "Kh√¥ng th·ªÉ t·∫£i chi ti·∫øt."
        
        prompt = f"""Ph√¢n t√≠ch k·∫øt qu·∫£ b√†i l√†m:

TH√îNG TIN:
- ƒêi·ªÉm: {attempt.get('score', 0):.1f}/100
- ƒê√∫ng: {attempt.get('correctAnswers', 0)}/{attempt.get('totalQuestions', 0)}

CH·ª¶ ƒê·ªÄ Y·∫æU:
{chr(10).join([f"- {t.get('topic', 'N/A')}: {t.get('accuracy', 0):.1f}%" for t in weak_topics])}

{incorrect_answers_str}

TR·∫¢ V·ªÄ JSON:
{{
  "analysis": "...",
  "strengths": ["..."],
  "weaknesses": ["..."],
  "recommendations": ["..."],
  "suggestedTopics": ["..."]
}}"""
        
        response = model.generate_content(prompt)
        
        json_text = clean_json_response(response.text)
        if not json_text:
            raise HTTPException(status_code=500, detail="AI tr·∫£ v·ªÅ d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá")

        result = json.loads(json_text)
        return result
        
    except Exception as e:
        print(f"‚ùå Analyze error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== GENERATE ADAPTIVE TEST ENDPOINT =====

@app.post("/api/generate-adaptive-test")
async def handle_generate_adaptive_test(request: GenerateAdaptiveTestInput):
    try:
        model = get_test_model()
        topics_str = ", ".join(request.weakTopics)
        
        prompt = f"""T·∫°o ƒë·ªÅ thi th√≠ch ·ª©ng cho c√°c ch·ªß ƒë·ªÅ y·∫øu:

CH·ª¶ ƒê·ªÄ: {topics_str}
ƒê·ªô kh√≥: {request.difficulty}

Y√äU C·∫¶U:
- 70% c√¢u v·ªÅ ch·ªß ƒë·ªÅ y·∫øu
- 30% t·ªïng h·ª£p
- ƒê·ªô kh√≥ tƒÉng d·∫ßn
- T·∫§T C·∫¢ \\ ESCAPE: \\\\frac

JSON (KH√îNG markdown)."""
        
        response = model.generate_content(prompt)
        
        try:
            json_text = clean_json_response(response.text)
            if not json_text:
                raise ValueError("Kh√¥ng t√¨m th·∫•y JSON")
            result = json.loads(json_text)
        except Exception as e:
            raise HTTPException(status_code=500, detail="JSON kh√¥ng h·ª£p l·ªá")
        
        return {
            "userId": request.userId,
            "weakTopics": request.weakTopics,
            "test": result
        }
        
    except Exception as e:
        print(f"‚ùå Adaptive test error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== STARTUP =====

if __name__ == "__main__":
    print("\n" + "="*60)
    print("üöÄ Starting OPTIMIZED Math Tutor API")
    print("="*60)
    print(f"üìÅ Exercises: {EXERCISES_FOLDER}")
    print(f"üìÅ Tests: {TESTS_FOLDER}")
    print("\n‚ö° Optimizations:")
    print("  - Cached system instructions (5 models)")
    print("  - Singleton pattern (no re-initialization)")
    print("  - Compact prompts (90% smaller)")
    print("  - Session-based chat")
    print("\n‚è±Ô∏è  Expected speed:")
    print("  - First request: 2-3s")
    print("  - Follow-up: 0.8-1.2s (70% faster)")
    print("="*60 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)